{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.types as fot\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device settings\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "def load_model():\n",
    "    CHECKPOINT = \"microsoft/Florence-2-base-ft\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(CHECKPOINT, trust_remote_code=True).to(device, dtype=torch_dtype)\n",
    "    processor = AutoProcessor.from_pretrained(CHECKPOINT, trust_remote_code=True)\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor\n",
    "model, processor = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for a single image\n",
    "image = Image.open('tiny_coco_subset/data/000000000139.jpg').convert(\"RGB\")\n",
    "\n",
    "inputs = processor(text=\"<CAPTION>\", images=image, return_tensors=\"pt\").to(device, torch_dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        max_new_tokens=512,\n",
    "        num_beams=3,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "parsed_answer = processor.post_process_generation(\n",
    "    generated_text,\n",
    "    task=task_prompt,\n",
    "    image_size=(image.width, image.height)\n",
    ")\n",
    "\n",
    "caption = parsed_answer.get(\"<CAPTION>\", \"No caption generated.\")\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:41<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions saved to generated_captions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Processing for all images \n",
    "\n",
    "image_folder = \"tiny_coco_subset/data\"\n",
    "output_csv = \"generated_captions.csv\"\n",
    "\n",
    "image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "captions_data = []\n",
    "\n",
    "for image_file in tqdm(image_files):\n",
    "    try:\n",
    "        \n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        \n",
    "        inputs = processor(text=\"<CAPTION>\", images=image, return_tensors=\"pt\").to(device, torch_dtype)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                pixel_values=inputs[\"pixel_values\"],\n",
    "                max_new_tokens=512,\n",
    "                num_beams=3,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        \n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "        parsed_answer = processor.post_process_generation(\n",
    "            generated_text,\n",
    "            task=task_prompt,\n",
    "            image_size=(image.width, image.height)\n",
    "        )\n",
    "\n",
    "        \n",
    "        caption = parsed_answer.get(\"<CAPTION>\", \"No caption generated.\")\n",
    "\n",
    "        \n",
    "        captions_data.append([image_file, caption])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_file}: {e}\")\n",
    "        captions_data.append([image_file, \"ERROR\"])\n",
    "\n",
    "with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"image_file\", \"caption\"])\n",
    "    writer.writerows(captions_data)\n",
    "\n",
    "print(f\"Captions saved to {output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Oasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
